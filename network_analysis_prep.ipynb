{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "\n",
    "\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "path = os.environ['PROJECT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Metric to Analyse \n",
    "\n",
    "- **Clustering statistics** - Number of addresses: “Each data point refers to the totals observed up to a given block, with weekly frequency, and time “flows” from the bottom left to the top right of the panel.” ([Campajola et al., 2022, p. 2](zotero://select/library/items/RYAYFRFR))\n",
    "    \n",
    "- **Degree distributions** - “The degree of a node represents the number of counterparts that entity exchanges tokens with, and is typically considered a measure of importance within the network. What is particularly relevant is to consider the distribution of degrees, as its shape is a direct consequence of the way in which the networks form.” ([Campajola et al., 2022, p. 3](zotero://select/library/items/RYAYFRFR)) ([pdf](zotero://open-pdf/library/items/LMUZEWY6?page=3))\n",
    "    \n",
    "- **Core-periphery structure** - “This *[edit. core-periphery structure]* is a macroscopic property of the network, that presents a split between a minority of nodes (the “core”) with a strong connectivity between themselves and the remaining nodes of the network (the “periphery”) that are mostly connected to core nodes and have relatively few links to other peripheral nodes.” ([Campajola et al., 2022, p. 4](zotero://select/library/items/RYAYFRFR)) ([pdf](zotero://open-pdf/library/items/LMUZEWY6?page=4))\n",
    "    \n",
    "    - “weekly transaction networks time-series and then consider the size of the core group as a fraction of the total size of the network” ([Campajola et al., 2022, p. 4](zotero://select/library/items/RYAYFRFR)) ([pdf](zotero://open-pdf/library/items/LMUZEWY6?page=4))\n",
    "        \n",
    "    - Relative size is used to assess centralisation or decentralisation trend\n",
    "        \n",
    "- **Mining concentration** - “The Nakamoto index provides us with a measure of the system’s distance from a 51% attack” ([Campajola et al., 2022, p. 5](zotero://select/library/items/RYAYFRFR)) ([pdf](zotero://open-pdf/library/items/LMUZEWY6?page=5))\n",
    "    \n",
    "- **Wealth inequality and spatial distribution -** “We define as wealth the balance held by an entity (address or cluster of addresses) at a given point in time, representing the funds that they are entitled to spend according to the blockchain ledger.” ([Campajola et al., 2022, p. 6](zotero://select/library/items/RYAYFRFR))\n",
    "    \n",
    "    - Token distribution and Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "\n",
    "file = 'tx_all_uniq_addresses.edgelist'\n",
    "G = nx.read_edgelist(join(path, file), data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistically Validated Networks in Bipartite Complex Systems\n",
    "\n",
    "For the analysis we sub-divide the our data into two sub-set.\n",
    "\n",
    "- N_A = Set of 17 Token addresses \n",
    "    > |Note: Tokenmigration or wrapped version (e.g. wNXM) have been replace with the address of the corresponding address. \n",
    "- N_B = Remainder of 78700610 (78700627-17) addresses \n",
    "- k = 1 (always)\n",
    "    >  From Tumminello et al., 2011, p. 3 - *Set A of the database is composed by 66 organisms (13 Archaea, 50 Bacteria and 3 unicellular Eukaryota) and set B by 4,873 COGs present in their genomes. The number of COGs in a genome is heterogeneous, ranging from 362 to 2,243. Similarly, COGs can be present in a different number of genomes. We call any COG that is present in k different genomes a k-COG. In the present system, k ranges between 3 and 66.*\n",
    "    > By analogy Set A in our case tokens are homogenous sub-groups of 1 (different organism species)\n",
    "- N_t < 1*17*(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import itertools\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burner addresses \n",
    "known_burner_addresses = ['0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000001',\n",
    "                        '0x0000000000000000000000000000000000000002',\n",
    "                        '0x0000000000000000000000000000000000000003',\n",
    "                        '0x0000000000000000000000000000000000000004',\n",
    "                        '0x0000000000000000000000000000000000000005',\n",
    "                        '0x0000000000000000000000000000000000000006',\n",
    "                        '0x0000000000000000000000000000000000000007',\n",
    "                        '0x000000000000000000000000000000000000dead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read snapshot\n",
    "ddf = dd.read_csv(join(path, 'token_balance_lookup_tables/token_holder_snapshot_balance_14967365.csv'))\n",
    "\n",
    "# we only consider non-zero at snapshot \n",
    "ddf = ddf[ddf.value > 0]\n",
    "\n",
    "#  filter latest tokens \n",
    "df_addresses = pd.read_csv('assets/df_final_token_selection_20221209.csv')\n",
    "ddf = ddf[ddf.token_address.isin(df_addresses.address) == True]\n",
    "\n",
    "# remove known burner addresses \n",
    "ddf = ddf[ddf.address.isin(known_burner_addresses) == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom \n",
    "# hypergeom.cdf(x,M,n,N,loc=0)\t\n",
    "\n",
    "import pandas as pd\n",
    "def main(address1, address2, pop_size):\n",
    "\n",
    "    # get unique addresses \n",
    "    token1_uniqa = ddf[ddf.token_address == address1].address.unique()\n",
    "    token2_uniqa = ddf[ddf.token_address == address2].address.unique()\n",
    "\n",
    "    # calcluate intersection \n",
    "    token1_token2_uniqa_intersection = np.intersect1d(token1_uniqa,token2_uniqa, assume_unique=True)\n",
    "\n",
    "    # calcualte number \n",
    "    len_token1 = len(token1_uniqa)\n",
    "    len_token2 = len(token2_uniqa)\n",
    "    len_intersection = len(token1_token2_uniqa_intersection)\n",
    "\n",
    "    # calculate hyptoge\n",
    "\n",
    "    # Define the parameters of the distribution\n",
    "    M = pop_size  # population size\n",
    "    n = len_token1  # number of draws\n",
    "    K = len_token2  # number of successes in population\n",
    "    x = len_intersection    # number of successes in draws\n",
    "\n",
    "    # Compute the cumulative probability of obtaining at most x successes\n",
    "    pvalue = 1 - hypergeom.cdf(x, M, n, K)\n",
    "    \n",
    "    print(f'token_address {address1} has {len_token1} Unique Addresses | token_address {address2} has {len_token2} Unique Addresses | Intersection: {len_intersection}, p value: {pvalue}')\n",
    "\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Demo\n",
    "# # test tokens \n",
    "# test_token1 = '0x111111111117dc0aa78b770fa6a738034120c302'\n",
    "# test_token2 = '0x0bc529c00c6401aef6d220be8c6ea1667f6ad93e'\n",
    "\n",
    "# # calc pop_size\n",
    "# pop_size = len(ddf.address.unique()) \n",
    "\n",
    "# # main\n",
    "# main(test_token1,test_token2, pop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = len(ddf.address.unique()) \n",
    "p_dict = {}\n",
    "\n",
    "for combination in itertools.combinations(df_addresses.address, 2): \n",
    "\n",
    "    pvalue = main(combination[0],combination[1], pop_size)\n",
    "\n",
    "    p_dict[combination] = pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate p values \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pvalues = pd.DataFrame.from_dict(p_dict, orient='index')\n",
    "\n",
    "df_pvalues.reset_index(inplace=True)\n",
    "\n",
    "df_pvalues.columns =  ['combination', 'p_value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests as m_tests\n",
    "# Ref.: https://www.statsmodels.org/devel/generated/statsmodels.stats.multitest.multipletests.html#statsmodels.stats.multitest.multipletests\n",
    "m_test = m_tests(pvals=df_pvalues.p_value, alpha=0.01, method='bonferroni')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pvalues['m_test_result'] = m_test[0]\n",
    "df_pvalues['m_test_value'] = m_test[1]\n",
    "df_pvalues.to_csv('pvalues_snapshot_14967365.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "df_pvalues = pd.read_csv('pvalues_snapshot_14967365.csv')\n",
    "\n",
    "df_pvalues_validated = df_pvalues[df_pvalues.m_test_result == True]\n",
    "df_pvalues_validated.combination = df_pvalues_validated.combination.apply(lambda x: literal_eval(x))  \n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add the edges to the graph\n",
    "G.add_edges_from(df_pvalues_validated.combination)\n",
    "\n",
    "# create labels \n",
    "df_a_fil = df_addresses[df_addresses.address.isin(list(G.nodes()))]\n",
    "labels = df_a_fil[['address', 'name']].set_index('address').to_dict()['name']\n",
    "\n",
    "# visualise netwotk \n",
    "nx.draw(G, labels=labels)\n",
    "\n",
    "#show\n",
    "plt.show()\n",
    "plt.savefig('vNetwork_14967365.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify basic network metrics "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of nodes and edges in the graph. This can give you a sense of the size and complexity of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The degree distribution of the graph. This describes the number of connections (or \"degrees\") that each node has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## degree \n",
    "degrees = dict(G.degree())\n",
    "# Calculate the average degree of the graph\n",
    "avg_degree = sum(degrees.values()) / len(degrees)\n",
    "print(avg_degree)\n",
    "\n",
    "\n",
    "# Calculate the maximum degree of the graph\n",
    "max_degree = max(degrees.values())\n",
    "print(max_degree)\n",
    "\n",
    "# Calculate the minimum degree of the graph\n",
    "min_degree = min(degrees.values())\n",
    "print(min_degree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diameter and average shortest path length of the graph. The diameter is the longest shortest path between any two nodes in the graph, and the average shortest path length is the average of all the shortest paths in the graph. These statistics can give you a sense of how \"connected\" the graph is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diameter\n",
    "nx.diameter(G)\n",
    "\n",
    "# average shortest path\n",
    "nx.average_shortest_path_length(G)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The density of the graph. This is the ratio of the number of edges in the graph to the maximum number of edges that the graph could have. A dense graph has a high number of edges, while a sparse graph has a low number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The degree centrality and betweenness centrality of the nodes in the graph. The degree centrality of a node is a measure of how many connections it has, and the betweenness centrality of a node is a measure of how important it is for connecting other nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality \n",
    "g_degree_centrality = dict(nx.degree_centrality(G))\n",
    "sum(g_degree_centrality.values())/ len(g_degree_centrality)\n",
    "\n",
    "\n",
    "# betweeness \n",
    "g_betweeness_centrality = dict(nx.betweenness_centrality(G))\n",
    "sum(g_betweeness_centrality.values())/ len(g_betweeness_centrality)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clustering coefficient - Algorithms to characterize the number of triangles in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_triangles = dict(nx.triangles(G))\n",
    "sum(g_triangles.values())/ len(g_triangles)\n",
    "\n",
    "g_clustering = dict(nx.clustering(G))\n",
    "sum(g_clustering.values())/ len(g_clustering)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat for other snapshot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import itertools\n",
    "\n",
    "from os.path import join\n",
    "from scipy.stats import hypergeom \n",
    "from statsmodels.stats.multitest import multipletests as m_tests\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "summary_stats = {}\n",
    "\n",
    "\n",
    "progress = 0 \n",
    "\n",
    "# snapshot selection \n",
    "df_snapshot = pd.read_csv('assets/snapshot_selection.csv')\n",
    "\n",
    "# address selection \n",
    "df_addresses = pd.read_csv('assets/df_final_token_selection_20221209.csv')\n",
    "\n",
    "# burner addresses \n",
    "# remove burner addresses \n",
    "known_burner_addresses = ['0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000001',\n",
    "                        '0x0000000000000000000000000000000000000002',\n",
    "                        '0x0000000000000000000000000000000000000003',\n",
    "                        '0x0000000000000000000000000000000000000004',\n",
    "                        '0x0000000000000000000000000000000000000005',\n",
    "                        '0x0000000000000000000000000000000000000006',\n",
    "                        '0x0000000000000000000000000000000000000007',\n",
    "                        '0x000000000000000000000000000000000000dead']\n",
    "\n",
    "\n",
    "\n",
    "#### functions ####\n",
    "\n",
    "def main(address1, address2, pop_size):\n",
    "\n",
    "    # get unique addresses \n",
    "    token1_uniqa = ddf[ddf.token_address == address1].address.unique()\n",
    "    token2_uniqa = ddf[ddf.token_address == address2].address.unique()\n",
    "\n",
    "    # calcluate intersection \n",
    "    token1_token2_uniqa_intersection = np.intersect1d(token1_uniqa,token2_uniqa, assume_unique=True)\n",
    "\n",
    "    # calcualte number \n",
    "    len_token1 = len(token1_uniqa)\n",
    "    len_token2 = len(token2_uniqa)\n",
    "    len_intersection = len(token1_token2_uniqa_intersection)\n",
    "\n",
    "    # calculate hyptoge\n",
    "\n",
    "    # Define the parameters of the distribution\n",
    "    M = pop_size  # population size\n",
    "    n = len_token1  # number of draws\n",
    "    K = len_token2  # number of successes in population\n",
    "    x = len_intersection    # number of successes in draws\n",
    "\n",
    "    # Compute the cumulative probability of obtaining at most x successes\n",
    "    pvalue = 1 - hypergeom.cdf(x, M, n, K)\n",
    "    \n",
    "    # print(f'token_address {address1} has {len_token1} Unique Addresses | token_address {address2} has {len_token2} Unique Addresses | Intersection: {len_intersection} | p value: {pvalue}')\n",
    "\n",
    "    return pvalue\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "for snapshot in df_snapshot['Block Height']:\n",
    "\n",
    "    # Info\n",
    "    items_left = len(df_snapshot) - progress\n",
    "    progress += 1\n",
    "    print(\n",
    "        f\"Current Snapshot: {snapshot} || Items processed: {progress} || Items left: { (items_left) }\"\n",
    "    )\n",
    "\n",
    "    ## formating of data\n",
    "    # load data\n",
    "    ddf = dd.read_csv(join(path, f'token_balance_lookup_tables/token_holder_snapshot_balance_{snapshot}.csv'))\n",
    "\n",
    "    # filter data \n",
    "    ddf = ddf[ddf.value > 0]\n",
    "    ddf = ddf[ddf.token_address.isin(df_addresses.address) == True]\n",
    "\n",
    "    # remove known burner addresses \n",
    "    ddf = ddf[ddf.address.isin(known_burner_addresses) == False]\n",
    "\n",
    "    # population size\n",
    "    pop_size = len(ddf.address.unique()) \n",
    "    p_dict = {}\n",
    "\n",
    "    # reduce address list \n",
    "    \n",
    "    present_addresses = list(ddf.token_address.unique().compute())\n",
    "\n",
    "    if len(present_addresses) == 1:\n",
    "\n",
    "        print('One address only')\n",
    "        stats = {'nodes': np.nan, 'possible_nodes': len(present_addresses), 'edges': np.nan, 'avg_degree_path': np.nan, 'min_degree_path': np.nan, 'max_degree_path': np.nan,'diameter': np.nan, 'avg_shortest_path': np.nan, 'density': np.nan, 'degree_centrality_avg': np.nan, 'degree_centrality_min': np.nan, 'degree_centrality_max': np.nan, 'betweeness_centrality_avg': np.nan, 'betweeness_centrality_min': np.nan, 'betweeness_centrality_max': np.nan, 'triangles_avg': np.nan, 'triangles_min': np.nan, 'triangles_max': np.nan}\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            # iterations \n",
    "            for combination in itertools.combinations(present_addresses, 2): \n",
    "\n",
    "                pvalue = main(combination[0],combination[1], pop_size)\n",
    "\n",
    "                p_dict[combination] = pvalue\n",
    "\n",
    "            ## Evaluate pvalues\n",
    "            # store pvalues \n",
    "            df_pvalues = pd.DataFrame.from_dict(p_dict, orient='index')\n",
    "            df_pvalues.reset_index(inplace=True)\n",
    "            df_pvalues.columns =  ['combination', 'p_value']\n",
    "\n",
    "            # value test \n",
    "            m_test = m_tests(pvals=df_pvalues.p_value, alpha=0.01, method='bonferroni')\n",
    "            df_pvalues['m_test_result'] = m_test[0]\n",
    "            df_pvalues['m_test_value'] = m_test[1]\n",
    "            df_pvalues.to_csv(join(path, f'output/pvalues_{snapshot}.csv'))\n",
    "\n",
    "\n",
    "            ## Build graph\n",
    "            # filter df  \n",
    "            df_pvalues_validated = df_pvalues[df_pvalues.m_test_result == True]\n",
    "\n",
    "            # Create an empty graph\n",
    "            G = nx.Graph()\n",
    "\n",
    "            # Add the edges to the graph\n",
    "            G.add_edges_from(df_pvalues_validated.combination)\n",
    "\n",
    "            # create labels \n",
    "            df_a_fil = df_addresses[df_addresses.address.isin(list(G.nodes()))]\n",
    "            labels = df_a_fil[['address', 'name']].set_index('address').to_dict()['name']\n",
    "\n",
    "            # visualise netwotk \n",
    "            nx.draw(G, labels=labels)\n",
    "\n",
    "            #show\n",
    "            plt.savefig(join(path, f'output/pics/pic_vNetwork_{snapshot}.png'))\n",
    "\n",
    "            # clear img\n",
    "            plt.clf() \n",
    "\n",
    "            ## descriptve statistic\n",
    "            g_nodes = G.number_of_nodes()\n",
    "            g_edges = G.number_of_edges()\n",
    "            ## degree \n",
    "            g_degrees = dict(G.degree())\n",
    "            # Calculate the average degree of the graph\n",
    "\n",
    "            try: \n",
    "                g_avg_degree = sum(g_degrees.values()) / len(g_degrees)\n",
    "                g_max_degree = max(g_degrees.values())\n",
    "                g_min_degree = min(g_degrees.values())\n",
    "            except: \n",
    "                g_avg_degree = np.nan\n",
    "                g_max_degree = np.nan\n",
    "                g_min_degree = np.nan\n",
    "\n",
    "\n",
    "            # diameter\n",
    "            g_diameter = nx.diameter(G)\n",
    "\n",
    "            # average shortest path\n",
    "            g_avg_shortest_path = nx.average_shortest_path_length(G)\n",
    "\n",
    "\n",
    "            g_density = nx.density(G)\n",
    "\n",
    "            # centrality \n",
    "            g_degree_centrality = dict(nx.degree_centrality(G))\n",
    "\n",
    "            try: \n",
    "                g_degree_centrality_avg = sum(g_degree_centrality.values())/ len(g_degree_centrality)\n",
    "                g_degree_centrality_min = min(g_degree_centrality.values())\n",
    "                g_degree_centrality_max = max(g_degree_centrality.values())\n",
    "\n",
    "            except:\n",
    "\n",
    "                g_degree_centrality_avg = np.nan\n",
    "                g_degree_centrality_min = np.nan\n",
    "                g_degree_centrality_max = np.nan\n",
    "\n",
    "            # betweeness \n",
    "            g_betweeness_centrality = dict(nx.betweenness_centrality(G))\n",
    "\n",
    "            try: \n",
    "                g_betweeness_centrality_avg = sum(g_betweeness_centrality.values())/ len(g_betweeness_centrality)\n",
    "                g_betweeness_centrality_min = min(g_betweeness_centrality.values())\n",
    "                g_betweeness_centrality_max = max(g_betweeness_centrality.values())\n",
    "\n",
    "            except:\n",
    "                g_betweeness_centrality_avg = np.nan\n",
    "                g_betweeness_centrality_min = np.nan\n",
    "                g_betweeness_centrality_max = np.nan\n",
    "\n",
    "            # triangles \n",
    "            g_triangles = dict(nx.triangles(G))\n",
    "\n",
    "            try: \n",
    "                g_triangles_avg = sum(g_triangles.values())/ len(g_triangles)\n",
    "                g_triangles_min = min(g_triangles.values())\n",
    "                g_triangles_max = max(g_triangles.values())\n",
    "            except: \n",
    "                g_triangles_avg = np.nan\n",
    "                g_triangles_min = np.nan\n",
    "                g_triangles_max = np.nan\n",
    "\n",
    "            # g_clustering = dict(nx.clustering(G))\n",
    "            # g_clustering_avg = sum(g_clustering.values())/ len(g_clustering)\n",
    "            # g_clustering_min = min(g_clustering.values())\n",
    "            # g_clustering_max = max(g_clustering.values())\n",
    "\n",
    "            stats = {'nodes': g_nodes, 'possible_nodes': len(present_addresses), 'edges': g_edges, 'avg_degree_path': g_avg_degree, 'min_degree_path': g_min_degree, 'max_degree_path': g_max_degree,'diameter': g_diameter, 'avg_shortest_path': g_avg_shortest_path, 'density': g_density, 'degree_centrality_avg': g_degree_centrality_avg, 'degree_centrality_min': g_degree_centrality_min, 'degree_centrality_max': g_degree_centrality_max, 'betweeness_centrality_avg': g_betweeness_centrality_avg, 'betweeness_centrality_min': g_betweeness_centrality_min, 'betweeness_centrality_max': g_betweeness_centrality_max, 'triangles_avg': g_triangles_avg, 'triangles_min': g_triangles_min, 'triangles_max': g_triangles_max}\n",
    "            # 'clustering_avg': g_clustering_avg, 'clustering_min': g_clustering_min, 'clustering_max': g_clustering_max }\n",
    "\n",
    "        except: \n",
    "            stats = {'nodes': np.nan, 'possible_nodes': len(present_addresses), 'edges': np.nan, 'avg_degree_path': np.nan, 'min_degree_path': np.nan, 'max_degree_path': np.nan,'diameter': np.nan, 'avg_shortest_path': np.nan, 'density': np.nan, 'degree_centrality_avg': np.nan, 'degree_centrality_min': np.nan, 'degree_centrality_max': np.nan, 'betweeness_centrality_avg': np.nan, 'betweeness_centrality_min': np.nan, 'betweeness_centrality_max': np.nan, 'triangles_avg': np.nan, 'triangles_min': np.nan, 'triangles_max': np.nan}\n",
    "    \n",
    "\n",
    "summary_stats[snapshot] = stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## descriptve statistic\n",
    "g_nodes = G.number_of_nodes()\n",
    "g_edges = G.number_of_edges()\n",
    "## degree \n",
    "g_degrees = dict(G.degree())\n",
    "# Calculate the average degree of the graph\n",
    "g_avg_degree = sum(g_degrees.values()) / len(g_degrees)\n",
    "g_max_degree = max(g_degrees.values())\n",
    "g_min_degree = min(g_degrees.values())\n",
    "\n",
    "\n",
    "# diameter\n",
    "g_diameter = nx.diameter(G)\n",
    "\n",
    "# average shortest path\n",
    "g_avg_shortest_path = nx.average_shortest_path_length(G)\n",
    "\n",
    "\n",
    "g_density = nx.density(G)\n",
    "\n",
    "# centrality \n",
    "g_degree_centrality = dict(nx.degree_centrality(G))\n",
    "g_degree_centrality_avg = sum(g_degree_centrality.values())/ len(g_degree_centrality)\n",
    "g_degree_centrality_min = min(g_degree_centrality.values())\n",
    "g_degree_centrality_max = max(g_degree_centrality.values())\n",
    "\n",
    "# betweeness \n",
    "g_betweeness_centrality = dict(nx.betweenness_centrality(G))\n",
    "g_betweeness_centrality_avg = sum(g_betweeness_centrality.values())/ len(g_betweeness_centrality)\n",
    "g_betweeness_centrality_min = min(g_betweeness_centrality.values())\n",
    "g_betweeness_centrality_max = max(g_betweeness_centrality.values())\n",
    "\n",
    "# triangles \n",
    "g_triangles = dict(nx.triangles(G))\n",
    "g_triangles_avg = sum(g_triangles.values())/ len(g_triangles)\n",
    "g_triangles_min = min(g_triangles.values())\n",
    "g_triangles_max = max(g_triangles.values())\n",
    "\n",
    "g_clustering = dict(nx.clustering(G))\n",
    "g_clustering_avg = sum(g_clustering.values())/ len(g_clustering)\n",
    "g_clustering_min = min(g_clustering.values())\n",
    "g_clustering_max = max(g_clustering.values())\n",
    "\n",
    "stats = {'nodes': g_nodes, 'edges': g_edges, \n",
    "'avg_degree_path': g_avg_degree, 'min_degree_path': g_min_degree, 'max_degree_path': g_max_degree,\n",
    "'diameter': g_diameter,\n",
    "'avg_shortest_path': g_avg_shortest_path, 'density': g_density,\n",
    "'degree_centrality_avg': g_degree_centrality_avg, 'degree_centrality_min': g_degree_centrality_min, 'degree_centrality_max': g_degree_centrality_max, \n",
    "'betweeness_centrality_avg': g_betweeness_centrality_avg, 'betweeness_centrality_min': g_betweeness_centrality_min, 'betweeness_centrality_max': g_betweeness_centrality_max,\n",
    "'triangles_avg': g_triangles_avg, 'triangles_min': g_triangles_min, 'triangles_max': g_triangles_max,\n",
    "'clustering_avg': g_clustering_avg, 'clustering_min': g_clustering_min, 'clustering_max': g_clustering_max }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= {}\n",
    "\n",
    "test[123] = stats\n",
    "test[432] = stats "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
